---
title: "Modelos de Árboles"
output: html_document
date: '2022-03-22'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modelos de Árboles

Vamos a ver un ejmeplo de árboles binarios de regresión para predicción. 

```{r Arboles Binarios construccion de la base de datos}
require(tree)
require(SLBDD)
library(dplyr)
Arboles_orig <- read.csv("2019-08.csv")
Arboles_orig_sinvars<-Arboles_orig%>%slice(-c(2:13,726:729))%>%select(-c(59,61,96,105,124,129))

###Chequeo sin NA's
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==1)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==2)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==3)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==4)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==5)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==6)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==7)))

####Construcción de la transformación

Arboles_orig_sinvars_1<-Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==1)%>%slice(-c(1,711,712))
dim(Arboles_orig_sinvars_1)[1]

Arboles_orig_sinvars_2<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==2)%>%slice(-1), diff, lag=1))%>%slice(-711)
dim(Arboles_orig_sinvars_2)[1]

Arboles_orig_sinvars_3<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==3)%>%slice(-1), diff, lag=2))  ###No hay variables para hacer la transformación 3
dim(Arboles_orig_sinvars_3)[1]


Arboles_orig_sinvars_4<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==4)%>%slice(-1), log)) %>%slice(-c(711,712))
dim(Arboles_orig_sinvars_4)[1]

Arboles_orig_sinvars_5<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==5)%>%slice(-1), function(x) diff(log(x)))) %>%slice(-711)
dim(Arboles_orig_sinvars_5)[1]

Arboles_orig_sinvars_6<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==6)%>%slice(-1), function(x) diff(log(x),2))) 
dim(Arboles_orig_sinvars_6)[1]


Arboles_orig_sinvars_7<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==7)%>%slice(-1), function(x) diff((x/lag(x))-1)))%>%slice(-1)
dim(Arboles_orig_sinvars_7)[1]
###Confirmación de que están todas las columnas
dim(Arboles_orig_sinvars_1)[2]+dim(Arboles_orig_sinvars_2)[2]+dim(Arboles_orig_sinvars_3)[2]+dim(Arboles_orig_sinvars_4)[2]+dim(Arboles_orig_sinvars_5)[2]+dim(Arboles_orig_sinvars_6)[2]+dim(Arboles_orig_sinvars_7)[2]


####Combinando los data.frame
Fechas=Arboles_orig_sinvars$sasdate[2:711]
Fred_tibble<- tibble(cbind(Fechas,Arboles_orig_sinvars_1,Arboles_orig_sinvars_2,Arboles_orig_sinvars_4,Arboles_orig_sinvars_5,Arboles_orig_sinvars_6,Arboles_orig_sinvars_7))


```


```{r Graficos y exploratorios}
######Algunos aspectos exploratorios
#x=as.matrix(cbind(Arboles_orig_sinvars_1,Arboles_orig_sinvars_2,Arboles_orig_sinvars_4,Arboles_orig_sinvars_5,Arboles_orig_sinvars_6,Arboles_orig_sinvars_7))
FREDMDApril19<-read.csv("FREDMDApril19.csv",header = TRUE)
x=as.matrix(FREDMDApril19)
require(MTS)
y=scale(x,scale=TRUE) ##Se estandarizan los datos para que sean dibujados
tdx=c(1:710/12+1960)  #creación del índice
MTSplot(y,tdx)
```


```{r Arboles 1}
require(tree)
Y=x[7:710,6] # variable dependiente o de respuesta univariada
X=cbind(x[6:709,],x[5:708,],x[4:707,],x[3:706,],x[2:705,],x[1:704,])
X1 <- scale(X,center=TRUE,scale=TRUE)
tree.fit <- tree(Y~.,data=data.frame(X1),split="deviance")
tree.fit

summary(tree.fit)

plot(tree.fit)
text(tree.fit,cex=0.5)
```
### Bosques Aletorios
Vamos a considerar el mismo ejemplo del modelo árboles anterior, en este caso tenemos $n=704$ observaciones y $k=732$ variables. Los predictores están estandarizados. El argumento ntree controla el número de árboles, mientras que mtry controla el número de predictores a seleccionarse en cada partición, es decir $g$, en la notación que manejamos. Por defecto, el crecimiento del árbol se deja bajo la condición que que el número de datos puntuales no sean inferior a 5. Eso se puede cambiar usando el argumento $nodsize$ y $maxnodes$.  

```{r random forest}
require(randomForest)
set.seed(117)
rf.fit <- randomForest(X1,Y,ntree=500,mtry=20,importance=TRUE)
rf.fit


plot(rf.fit) ###Esta gráfica permite ver la disminución del ECM de predicción al incrementar el número de árboles.
dim(rf.fit$importance)

plot(1:732,rf.fit$importance[,2],ylab="mean decrease in MSE")   ##La gráfica muestra la disminución medio el EMC de pronóstico de cada predictor. 

varImpPlot(rf.fit)   ###Importancia de las variables

require(pdp)
require(vip)
vip(rf.fit, bar = FALSE, horizontal = FALSE, size = 1.5)
partialPlot(rf.fit,data.frame(cbind(Y,X1)),"USGOOD") ##representación gráfica del efecto marginal de una variable en la respuesta
```

Tarea:Qué sucede si aumentamos el número de predictoras en las particiones?

Hacer la mejora de requerirse usando la metodología que se menciona enseguida!

Recuerde que en el archivo Arboles.Rmd de series univariadas se encuentra 


## Random Forest con remuestreo temporal 
La idea es usar el reposotorio en github https://github.com/hyanworkspace/rangerts
donde se está el paquete rangerts que permite usar random forest para el contexto de series de tiempo, el cual está basado en el paquete ranger.

```{r Rf temporal}
## devtools::install_github("hyanworkspace/rangerts", quiet = T) ## instalación del paquete
library(rangerts)
```

