---
title: "Modelos de Árboles"
output: html_document
date: '2022-03-22'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Modelos de Árboles

Vamos a ver un ejmeplo de árboles binarios de regresión para predicción. 

```{r Arboles Binarios construccion de la base de datos}
require(tree)
require(SLBDD)
library(dplyr)
Arboles_orig <- read.csv("2019-08.csv")
Arboles_orig_sinvars<-Arboles_orig%>%slice(-c(2:13,726:729))%>%select(-c(59,61,96,105,124,129))

###Chequeo sin NA's
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==1)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==2)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==3)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==4)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==5)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==6)))
which(is.na(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==7)))

####Construcción de la transformación

Arboles_orig_sinvars_1<-Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==1)%>%slice(-c(1,711,712))
dim(Arboles_orig_sinvars_1)[1]

Arboles_orig_sinvars_2<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==2)%>%slice(-1), diff, lag=1))%>%slice(-711)
dim(Arboles_orig_sinvars_2)[1]

Arboles_orig_sinvars_3<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==3)%>%slice(-1), diff, lag=2))  ###No hay variables para hacer la transformación 3
dim(Arboles_orig_sinvars_3)[1]


Arboles_orig_sinvars_4<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==4)%>%slice(-1), log)) %>%slice(-c(711,712))
dim(Arboles_orig_sinvars_4)[1]

Arboles_orig_sinvars_5<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==5)%>%slice(-1), function(x) diff(log(x)))) %>%slice(-711)
dim(Arboles_orig_sinvars_5)[1]

Arboles_orig_sinvars_6<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==6)%>%slice(-1), function(x) diff(log(x),2))) 
dim(Arboles_orig_sinvars_6)[1]


Arboles_orig_sinvars_7<-as.data.frame(lapply(Arboles_orig_sinvars%>%select_if(Arboles_orig_sinvars[1,]==7)%>%slice(-1), function(x) diff((x/lag(x))-1)))%>%slice(-1)
dim(Arboles_orig_sinvars_7)[1]
###Confirmación de que están todas las columnas
dim(Arboles_orig_sinvars_1)[2]+dim(Arboles_orig_sinvars_2)[2]+dim(Arboles_orig_sinvars_3)[2]+dim(Arboles_orig_sinvars_4)[2]+dim(Arboles_orig_sinvars_5)[2]+dim(Arboles_orig_sinvars_6)[2]+dim(Arboles_orig_sinvars_7)[2]


####Combinando los data.frame
Fechas=Arboles_orig_sinvars$sasdate[2:711]
Fred_tibble<- tibble(cbind(Fechas,Arboles_orig_sinvars_1,Arboles_orig_sinvars_2,Arboles_orig_sinvars_4,Arboles_orig_sinvars_5,Arboles_orig_sinvars_6,Arboles_orig_sinvars_7))


```


```{r Graficos y exploratorios}
######Algunos aspectos exploratorios
#x=as.matrix(cbind(Arboles_orig_sinvars_1,Arboles_orig_sinvars_2,Arboles_orig_sinvars_4,Arboles_orig_sinvars_5,Arboles_orig_sinvars_6,Arboles_orig_sinvars_7))
FREDMDApril19<-read.csv("FREDMDApril19.csv",header = TRUE)
x=as.matrix(FREDMDApril19)
require(MTS)
y=scale(x,scale=TRUE) ##Se estandarizan los datos para que sean dibujados
tdx=c(1:710/12+1960)  #creación del índice
MTSplot(y,tdx)
```


```{r Arboles 1}
require(tree)
Y=x[7:710,6] # variable dependiente o de respuesta univariada
X=cbind(x[6:709,],x[5:708,],x[4:707,],x[3:706,],x[2:705,],x[1:704,])
X1 <- scale(X,center=TRUE,scale=TRUE)
tree.fit <- tree(Y~.,data=data.frame(X1),split="deviance")
tree.fit

summary(tree.fit)

plot(tree.fit)
text(tree.fit,cex=0.5)
dim(X1)
n_entre=600
X1.train=X1[1:n_entre,]
Y.train=Y[1:n_entre]

X1.test=X1[(n_entre+1):dim(X1)[1],]
Y.test=Y[(n_entre+1):dim(X1)[1]]
```
### Bosques Aletorios
Vamos a considerar el mismo ejemplo del modelo árboles anterior, en este caso tenemos $n=704$ observaciones y $k=732$ variables. Los predictores están estandarizados. El argumento ntree controla el número de árboles, mientras que mtry controla el número de predictores a seleccionarse en cada partición, es decir $g$, en la notación que manejamos. Por defecto, el crecimiento del árbol se deja bajo la condición que que el número de datos puntuales no sean inferior a 5. Eso se puede cambiar usando el argumento $nodsize$ y $maxnodes$.  

```{r random forest}
require(randomForest)
set.seed(117)
rf.fit <- randomForest(X1.train,Y.train,ntree=500,mtry=20,importance=TRUE)
rf.fit


plot(rf.fit) ###Esta gráfica permite ver la disminución del ECM de predicción al incrementar el número de árboles.
```

```{r Importancia y Grácias de dependencia parcial}

dim(rf.fit$importance)

plot(1:732,rf.fit$importance[,2],ylab="mean decrease in MSE")   ##La gráfica muestra la disminución medio el EMC de pronóstico de cada predictor. 

varImpPlot(rf.fit)   ###Importancia de las variables

require(pdp)
require(vip)
vip(rf.fit, bar = FALSE, horizontal = FALSE, size = 1.5)
partialPlot(rf.fit,data.frame(cbind(Y.train,X1.train)),"USGOOD") ##representación gráfica del efecto marginal de una variable en la respuesta
```

```{r Como cambian el error cuadrático medio con respecto a la número de predictores}
K=30
oob.err=double(K)
test.err=double(K)

##mtry is number of Variables randomly chosen at each split
for(mtry in 1:K) 
{
  rf=randomForest(X1.train,Y.train,ntree=500,mtry=mtry,importance=TRUE)
  oob.err[mtry] = rf$mse[400] 
  
  pred<-predict(rf,X1.test) #Predictions on Test Set for each Tree
  test.err[mtry]= mean((Y.test - pred)^2) #Mean Squared Test Error
  
  cat(mtry," ") #printing the output to the console
}
oob.err
test.err
```

```{r Gráfico}
matplot(1:mtry , cbind(oob.err,test.err), pch=19 , col=c("red","blue"),type="b",ylab="Error Cuadrático Medio",xlab="Número de Predictores Considerados en cada división")
legend("top",legend=c("Error Intramuestra","Error en el conjunto de Prueba"),pch=19, col=c("red","blue"))
```


Tarea:Qué sucede si aumentamos el número de predictoras en las particiones?

Hacer la mejora de requerirse usando la metodología que se menciona enseguida!

Recuerde que en el archivo Arboles.Rmd de series univariadas se encuentra 


## Random Forest con remuestreo temporal 
La idea es usar el reposotorio en github https://github.com/hyanworkspace/rangerts
donde se está el paquete rangerts que permite usar random forest para el contexto de series de tiempo, el cual está basado en el paquete ranger.

```{r Rf temporal}
## devtools::install_github("hyanworkspace/rangerts", quiet = T) ## instalación del paquete
library(rangerts)
```

